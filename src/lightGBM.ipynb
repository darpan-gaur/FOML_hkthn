{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv('../data/preprocessed_train.csv')\n",
    "test_data = pd.read_csv('../data/preprocessed_test.csv')\n",
    "\n",
    "# train_data = pd.read_csv('../data/preprocessed_train_smote.csv')\n",
    "# test_data = pd.read_csv('../data/preprocessed_test_smote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'UID' column as the index\n",
    "train_data.set_index('UID', inplace=True)\n",
    "\n",
    "# Set the 'UID' column as the index\n",
    "test_data.set_index('UID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with less feature importance\n",
    "# cols_to_drop = ['AgricultureZoningCode', 'CropSpeciesVariety', 'LandUsageType',\n",
    "#        'ValuationYear', 'StorageAndFacilityCount', 'WaterAccessPoints',\n",
    "#        'MainIrrigationSystemCount', 'WaterAccessPointsCalc',\n",
    "#        'NationalRegionCode']\n",
    "\n",
    "# train_data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "# test_data.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for 'Target' column\n",
    "target_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply the mapping to the 'Target' column\n",
    "train_labels = train_data['Target'].map(target_mapping)\n",
    "\n",
    "# Drop the 'Target' column from the training data\n",
    "train_data = train_data.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pca to reduce the number of features\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=10)\n",
    "# train_data = pd.DataFrame(pca.fit_transform(train_data), index=train_data.index)\n",
    "# test_data = pd.DataFrame(pca.transform(test_data), index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    22514\n",
      "1    22514\n",
      "2    22514\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make train data with 22514 data of each class\n",
    "\n",
    "# Separate the data based on the target classes\n",
    "low_class = train_data[train_labels == 0]\n",
    "medium_class = train_data[train_labels == 1]\n",
    "high_class = train_data[train_labels == 2]\n",
    "\n",
    "# Get the number of samples in each class\n",
    "low_class_count = len(low_class)\n",
    "medium_class_count = len(medium_class)\n",
    "high_class_count = len(high_class)\n",
    "\n",
    "# Set the number of samples to be selected from each class\n",
    "num_samples = min(low_class_count, medium_class_count, high_class_count)\n",
    "\n",
    "# Randomly sample data from each class\n",
    "low_class_sample = low_class.sample(n=num_samples, random_state=seed)\n",
    "medium_class_sample = medium_class.sample(n=num_samples, random_state=seed)\n",
    "high_class_sample = high_class.sample(n=num_samples, random_state=seed)\n",
    "\n",
    "# Concatenate the sampled data\n",
    "train_data_sampled = pd.concat([low_class_sample, medium_class_sample, high_class_sample])\n",
    "\n",
    "# Separate the features and target variable\n",
    "X_sampled = train_data_sampled\n",
    "y_sampled = train_labels.loc[train_data_sampled.index]\n",
    "\n",
    "# Display the count of unique values in the target variable\n",
    "print(y_sampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60787, 20)\n",
      "X_valid shape: (6755, 20)\n",
      "y_train shape: (60787,)\n",
      "y_valid shape: (6755,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.05, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sampled, y_sampled, test_size=0.1, random_state=seed)\n",
    "# X_train, y_train = X_sampled, y_sampled\n",
    "# X_valid, y_valid = X_sampled, y_sampled\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2830\n",
      "[LightGBM] [Info] Number of data points in the train set: 60787, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.097494\n",
      "[LightGBM] [Info] Start training from score -1.099271\n",
      "[LightGBM] [Info] Start training from score -1.099073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.522348528468258\n",
      "Valid F1 Score: 0.447520355292376\n",
      "Train Accuracy: 0.522348528468258\n",
      "Valid Accuracy: 0.447520355292376\n"
     ]
    }
   ],
   "source": [
    "# initialize the models\n",
    "lgbm = LGBMClassifier(random_state=seed)\n",
    "\n",
    "# fit the model\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "train_preds = lgbm.predict(X_train)\n",
    "valid_preds = lgbm.predict(X_valid)\n",
    "\n",
    "# calculate the f1 score\n",
    "train_f1 = f1_score(y_train, train_preds, average='micro')\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='micro')\n",
    "\n",
    "# calculate the accuracy\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# display the f1 score and accuracy\n",
    "print(f\"Train F1 Score: {train_f1}\")\n",
    "print(f\"Valid F1 Score: {valid_f1}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Valid Accuracy: {valid_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           importance\n",
      "Latitude                          970\n",
      "Longitude                         930\n",
      "TotalTaxAssessed                  803\n",
      "AgriculturalPostalZone            774\n",
      "FieldEstablishedYear              774\n",
      "TaxAgrarianValue                  769\n",
      "RawLocationId                     663\n",
      "TaxLandValue                      654\n",
      "CultivatedAreaSqft1               586\n",
      "TotalValue                        564\n",
      "TotalCultivatedAreaSqft           504\n",
      "AgricultureZoningCode             297\n",
      "CropSpeciesVariety                186\n",
      "LandUsageType                     162\n",
      "ValuationYear                     111\n",
      "StorageAndFacilityCount            88\n",
      "WaterAccessPoints                  75\n",
      "MainIrrigationSystemCount          54\n",
      "WaterAccessPointsCalc              36\n",
      "NationalRegionCode                  0\n"
     ]
    }
   ],
   "source": [
    "# make feature importance dataframe\n",
    "feature_importance = pd.DataFrame(lgbm.feature_importances_, index=X_train.columns, columns=['importance'])\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# display the dataframe\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = lgbm.predict(test_data)\n",
    "\n",
    "# reshape to 1D array\n",
    "# test_preds = test_preds.ravel()\n",
    "\n",
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)\n",
    "\n",
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/output/lightgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
