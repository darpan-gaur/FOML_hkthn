{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv('../data/preprocessed_train.csv')\n",
    "test_data = pd.read_csv('../data/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'UID' column as the index\n",
    "train_data.set_index('UID', inplace=True)\n",
    "\n",
    "# Set the 'UID' column as the index\n",
    "test_data.set_index('UID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for 'Target' column\n",
    "target_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply the mapping to the 'Target' column\n",
    "train_labels = train_data['Target'].map(target_mapping)\n",
    "\n",
    "# Drop the 'Target' column from the training data\n",
    "train_data = train_data.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    22514\n",
      "1    22514\n",
      "2    22514\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make train data with 22514 data of each class\n",
    "\n",
    "# Separate the data based on the target classes\n",
    "low_class = train_data[train_labels == 0]\n",
    "medium_class = train_data[train_labels == 1]\n",
    "high_class = train_data[train_labels == 2]\n",
    "\n",
    "# Get the number of samples in each class\n",
    "low_class_count = len(low_class)\n",
    "medium_class_count = len(medium_class)\n",
    "high_class_count = len(high_class)\n",
    "\n",
    "# Set the number of samples to be selected from each class\n",
    "num_samples = min(low_class_count, medium_class_count, high_class_count)\n",
    "\n",
    "# Randomly sample data from each class\n",
    "low_class_sample = low_class.sample(n=num_samples, random_state=seed)\n",
    "medium_class_sample = medium_class.sample(n=num_samples, random_state=seed)\n",
    "high_class_sample = high_class.sample(n=num_samples, random_state=seed)\n",
    "\n",
    "# Concatenate the sampled data\n",
    "train_data_sampled = pd.concat([low_class_sample, medium_class_sample, high_class_sample])\n",
    "\n",
    "# Separate the features and target variable\n",
    "X_sampled = train_data_sampled\n",
    "y_sampled = train_labels.loc[train_data_sampled.index]\n",
    "\n",
    "# Display the count of unique values in the target variable\n",
    "print(y_sampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (67542, 20)\n",
      "X_valid shape: (67542, 20)\n",
      "y_train shape: (67542,)\n",
      "y_valid shape: (67542,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.05, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sampled, y_sampled, test_size=0.1, random_state=seed)\n",
    "# X_train, y_train = X_sampled, y_sampled\n",
    "# X_valid, y_valid = X_sampled, y_sampled\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5319\n",
      "Training F1 Score: 0.5310\n",
      "Validation Accuracy: 0.5319\n",
      "Validation F1 Score: 0.5310\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=5,\n",
    "    max_depth=10,\n",
    "    bootstrap=False,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy and F1 score on the training set\n",
    "train_preds = rf.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "\n",
    "# Accuracy and F1 score on the validation set\n",
    "valid_preds = rf.predict(X_valid)\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "\n",
    "# Display the accuracy and F1 score\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = rf.predict(test_data)\n",
    "\n",
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)\n",
    "\n",
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/output/randomForest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
