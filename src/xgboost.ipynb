{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv('../data/preprocessed_train.csv')\n",
    "test_data = pd.read_csv('../data/preprocessed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'UID' column as the index\n",
    "train_data.set_index('UID', inplace=True)\n",
    "\n",
    "# Set the 'UID' column as the index\n",
    "test_data.set_index('UID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for 'Target' column\n",
    "target_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply the mapping to the 'Target' column\n",
    "train_labels = train_data['Target'].map(target_mapping)\n",
    "\n",
    "# Drop the 'Target' column from the training data\n",
    "train_data = train_data.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    22514\n",
      "1    22514\n",
      "2    22514\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make train data with 22514 data of each class\n",
    "\n",
    "# Separate the data based on the target classes\n",
    "low_class = train_data[train_labels == 0]\n",
    "medium_class = train_data[train_labels == 1]\n",
    "high_class = train_data[train_labels == 2]\n",
    "\n",
    "# Get the number of samples in each class\n",
    "low_class_count = len(low_class)\n",
    "medium_class_count = len(medium_class)\n",
    "high_class_count = len(high_class)\n",
    "\n",
    "# Set the number of samples to be selected from each class\n",
    "num_samples = min(low_class_count, medium_class_count, high_class_count)\n",
    "\n",
    "# Randomly sample data from each class\n",
    "low_class_sample = low_class.sample(n=num_samples, random_state=seed)\n",
    "medium_class_sample = medium_class.sample(n=num_samples, random_state=seed)\n",
    "high_class_sample = high_class.sample(n=num_samples, random_state=seed)\n",
    "\n",
    "# Concatenate the sampled data\n",
    "train_data_sampled = pd.concat([low_class_sample, medium_class_sample, high_class_sample])\n",
    "\n",
    "# Separate the features and target variable\n",
    "X_sampled = train_data_sampled\n",
    "y_sampled = train_labels.loc[train_data_sampled.index]\n",
    "\n",
    "# Display the count of unique values in the target variable\n",
    "print(y_sampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60787, 20)\n",
      "X_valid shape: (6755, 20)\n",
      "y_train shape: (60787,)\n",
      "y_valid shape: (6755,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.05, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sampled, y_sampled, test_size=0.1, random_state=seed)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.7131788046786319\n",
      "Validation F1 Score: 0.43375277572168763\n",
      "Train Accuracy: 0.7131788046786319\n",
      "Validation Accuracy: 0.43375277572168763\n"
     ]
    }
   ],
   "source": [
    "# initialize the models\n",
    "# params = {\n",
    "#     'n_estimators': 200,\n",
    "#     'learning_rate': 0.2,\n",
    "#     'max_depth': 10,\n",
    "#     'min_child_weight': 3,\n",
    "#     'subsample': 0.6,\n",
    "#     'random_state': seed\n",
    "# }\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "train_preds = xgb.predict(X_train)\n",
    "valid_preds = xgb.predict(X_valid)\n",
    "\n",
    "# calculate the f1 score\n",
    "train_f1 = f1_score(y_train, train_preds, average='micro')\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='micro')\n",
    "\n",
    "# calculate the accuracy\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# display the f1 score and accuracy\n",
    "print(f\"Train F1 Score: {train_f1}\")\n",
    "print(f\"Validation F1 Score: {valid_f1}\")\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = xgb.predict(test_data)\n",
    "\n",
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)\n",
    "\n",
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/output/xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(random_state=seed),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'max_depth': [ 2, 3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1, \n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(**fixed_params),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'subsample': [0.6, 0.8, 0,9, 1.0],\n",
    "    'max_delta_step': [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(**fixed_params),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.6,\n",
    "    'max_delta_step': 1,\n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_model = XGBClassifier(**final_params)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "valid_preds = xgb_model.predict(X_valid)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# Display the F1 scores\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
