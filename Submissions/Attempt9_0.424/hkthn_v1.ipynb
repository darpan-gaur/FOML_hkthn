{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for each column\n",
    "missing_values = train_data.isnull().sum()\n",
    "\n",
    "# Create a DataFrame to store the count of missing values\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'MissingCount': missing_values.values\n",
    "})\n",
    "\n",
    "# Add a column to show the percentage of missing values\n",
    "missing_df['MissingPercentage'] = (missing_df['MissingCount'] / len(train_data)) * 100\n",
    "\n",
    "# Sort the DataFrame by the number of missing values in descending order\n",
    "missing_df.sort_values(by='MissingCount', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index for readability\n",
    "missing_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- null rate for dropping set to > 60%.\n",
    "- Try 30% and 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with missing percentage greater than 60%\n",
    "columns_to_drop = missing_df[missing_df['MissingPercentage'] > 10]['Column'].tolist()\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "train_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame shape after dropping columns\n",
    "print(f\"Updated shape of the DataFrame: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop same columns from test data\n",
    "test_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame shape after dropping columns\n",
    "print(f\"Updated shape of the DataFrame: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'UID' column as the index\n",
    "train_data.set_index('UID', inplace=True)\n",
    "\n",
    "# Display the updated DataFrame to confirm the change\n",
    "# print(train_data.head())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'UID' column as the index\n",
    "test_data.set_index('UID', inplace=True)\n",
    "\n",
    "# Display the updated DataFrame to confirm the change\n",
    "# print(test_data.head())\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for 'Target' column\n",
    "target_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "# Apply the mapping to the 'Target' column\n",
    "train_labels = train_data['Target'].map(target_mapping)\n",
    "\n",
    "# Display the first few rows of the labels to verify the mapping\n",
    "print(train_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['TownId','Target','DistrictId'])\n",
    "\n",
    "test_data = test_data.drop(columns=['TownId','DistrictId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    # Define the columns based on their type\n",
    "    categorical_columns = [\n",
    "        'HarvestProcessingType', 'SoilFertilityType', 'AgricultureZoningCode',\n",
    "        'ValuationYear', 'NationalRegionCode', 'StorageAndFacilityCount', 'RawLocationId',\n",
    "        'LandUsageType', 'CropSpeciesVariety', 'AgriculturalPostalZone'\n",
    "    ]\n",
    "    \n",
    "    median_columns = [\n",
    "        'FarmingUnitCount', 'FieldSizeSqft', 'CultivatedAreaSqft1', 'MainIrrigationSystemCount',\n",
    "        'FieldEstablishedYear', 'TotalTaxAssessed', 'TaxLandValue', 'TotalCultivatedAreaSqft',\n",
    "        'WaterAccessPoints', 'TaxAgrarianValue', 'TotalValue'\n",
    "    ]\n",
    "    \n",
    "    mean_columns = [\n",
    "        'WaterAccessPointsCalc', 'Longitude', 'Latitude'\n",
    "    ]\n",
    "    \n",
    "    # Convert categorical columns to 'object' type if necessary\n",
    "    for column in categorical_columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].astype('object')\n",
    "\n",
    "    # Fill missing values for categorical columns using mode\n",
    "    for column in categorical_columns:\n",
    "        if column in df.columns:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                try:\n",
    "                    mode_value = df[column].mode(dropna=True)[0] if not df[column].mode().empty else None\n",
    "                    if mode_value is not None:\n",
    "                        df[column].fillna(mode_value, inplace=True)\n",
    "                    else:\n",
    "                        print(f\"Warning: Could not find a mode for column {column}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while filling mode for column {column}: {e}\")\n",
    "    \n",
    "    # Fill missing values for numerical columns using median\n",
    "    for column in median_columns:\n",
    "        if column in df.columns and df[column].dtype in ['int64', 'float64']:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "    \n",
    "    # Fill missing values for numerical columns using mean\n",
    "    for column in mean_columns:\n",
    "        if column in df.columns and df[column].dtype in ['int64', 'float64']:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                mean_value = df[column].mean()\n",
    "                df[column].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fill missing values in the training data\n",
    "train_data = fill_missing_values(train_data)\n",
    "\n",
    "# Check if there are still missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(\"Missing values after filling:\\n\", missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values in test data\n",
    "test_data = fill_missing_values(test_data)\n",
    "\n",
    "# Check if there are still missing values\n",
    "missing_values = test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance using random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = train_data\n",
    "y = train_labels\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store the feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by feature importance in descending order\n",
    "feature_importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index for readability\n",
    "feature_importances_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(feature_importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with importance less than 0.01\n",
    "columns_to_drop = feature_importances_df[feature_importances_df['Importance'] < 0.015]['Feature'].tolist()\n",
    "\n",
    "# Drop the identified columns from the DataFrame\n",
    "train_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Drop the identified columns from test data\n",
    "test_data.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print count of unique values in y\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Agriculturepostalzone to int\n",
    "train_data['AgriculturalPostalZone'] = train_data['AgriculturalPostalZone'].astype(int)\n",
    "test_data['AgriculturalPostalZone'] = test_data['AgriculturalPostalZone'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add uid to train and test data and save cleaned data\n",
    "# train_data['UID'] = train_data.index\n",
    "# test_data['UID'] = test_data.index\n",
    "\n",
    "# train_data['Target'] = train_labels\n",
    "\n",
    "# train_data.to_csv(\"cleaned_train.csv\", index=False)\n",
    "# test_data.to_csv(\"cleaned_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features before training\n",
    "scaler = StandardScaler()\n",
    "train_data = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns, index=train_data.index)\n",
    "test_data = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns, index=test_data.index)\n",
    "\n",
    "# train_data_norm = scaler.fit_transform(train_data)\n",
    "# test_data_norm = scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train data with 22514 data of each class\n",
    "\n",
    "# Separate the data based on the target classes\n",
    "low_class = train_data[train_labels == 0]\n",
    "medium_class = train_data[train_labels == 1]\n",
    "high_class = train_data[train_labels == 2]\n",
    "\n",
    "# Get the number of samples in each class\n",
    "low_class_count = len(low_class)\n",
    "medium_class_count = len(medium_class)\n",
    "high_class_count = len(high_class)\n",
    "\n",
    "# Set the number of samples to be selected from each class\n",
    "num_samples = min(low_class_count, medium_class_count, high_class_count)\n",
    "\n",
    "# Randomly sample data from each class\n",
    "low_class_sample = low_class.sample(n=num_samples, random_state=seed)\n",
    "medium_class_sample = medium_class.sample(n=num_samples, random_state=seed)\n",
    "high_class_sample = high_class.sample(n=num_samples, random_state=seed)\n",
    "\n",
    "# Concatenate the sampled data\n",
    "train_data_sampled = pd.concat([low_class_sample, medium_class_sample, high_class_sample])\n",
    "\n",
    "# Separate the features and target variable\n",
    "X_sampled = train_data_sampled\n",
    "y_sampled = train_labels.loc[train_data_sampled.index]\n",
    "\n",
    "# Display the count of unique values in the target variable\n",
    "print(y_sampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.2, random_state=seed)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy and F1 score on the training set\n",
    "train_preds = rf_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "\n",
    "# Accuracy and F1 score on the validation set\n",
    "valid_preds = rf_model.predict(X_valid)\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "\n",
    "# Display the accuracy and F1 score\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = rf_model.predict(test_data)\n",
    "\n",
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)\n",
    "\n",
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest on full data\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model_full = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Fit the model\n",
    "rf_model_full.fit(train_data, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_preds = rf_model_full.predict(test_data)\n",
    "\n",
    "# convert predictions to target values\n",
    "target_mapping_inv = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping_inv)\n",
    "\n",
    "# Create a DataFrame with the 'UID' column and the predictions\n",
    "submission_df = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission_df.to_csv('submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes Classifier\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Fit the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy and F1 score on the training set\n",
    "train_preds = nb_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "\n",
    "# Accuracy and F1 score on the validation set\n",
    "valid_preds = nb_model.predict(X_valid)\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "\n",
    "# Display the accuracy and F1 score\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions on test data\n",
    "test_preds = nb_model.predict(test_data)\n",
    "\n",
    "# convert predictions to target values\n",
    "target_mapping_inv = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping_inv)\n",
    "\n",
    "# Create a DataFrame with the 'UID' column and the predictions\n",
    "submission_df = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission_df.to_csv('submission_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_clf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "# gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=seed)\n",
    "# ada_clf = AdaBoostClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "# # create a voting classifier with soft voting\n",
    "# ensemble_model = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('rf', rf_clf),\n",
    "#         ('gb', gb_clf),\n",
    "#         ('ada', ada_clf)\n",
    "#     ],\n",
    "#     voting='soft'\n",
    "# )\n",
    "\n",
    "# # Fit the model\n",
    "# ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# valid_preds = ensemble_model.predict(X_valid)\n",
    "\n",
    "# # Calculate the accuracy of the model\n",
    "# accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# # Calculate the F1 score of the model\n",
    "# f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# # Display the accuracy of the model\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize classifiers with a random seed\n",
    "seed = 42\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=seed)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=seed)\n",
    "log_reg_clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "svc_clf = SVC(kernel='rbf', probability=True, random_state=seed)  # SVM with soft voting\n",
    "nb_clf = GaussianNB()\n",
    "\n",
    "# Create a voting classifier with soft voting\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_clf),\n",
    "        ('gb', gb_clf),\n",
    "        ('ada', ada_clf),\n",
    "        ('log_reg', log_reg_clf),\n",
    "        ('svc', svc_clf),\n",
    "        ('nb', nb_clf)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_preds = ensemble_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "\n",
    "# Evaluate on the validation set\n",
    "valid_preds = ensemble_model.predict(X_valid)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "\n",
    "# Display the F1 scores\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = ensemble_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print count of unique values in predictions\n",
    "print(np.unique(test_preds, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_model = XGBClassifier(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "valid_preds = xgb_model.predict(X_valid)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# Display the F1 scores\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver_1 xgboost\n",
    "xgb_v1 = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    n_estimators=200,\n",
    "    max_depth=2,\n",
    "    random_state=seed\n",
    ") \n",
    "\n",
    "# Fit the model\n",
    "xgb_v1.fit(X_train, y_train)\n",
    "\n",
    "# get accuracy and f1 score on training set and validation set\n",
    "train_preds = xgb_v1.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "valid_preds = xgb_v1.predict(X_valid)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# Display the F1 scores\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(random_state=seed),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'max_depth': [ 2, 3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1, \n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(**fixed_params),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'subsample': [0.6, 0.8, 0,9, 1.0],\n",
    "    'max_delta_step': [0, 1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "csv = GridSearchCV(\n",
    "    estimator=XGBClassifier(**fixed_params),\n",
    "    param_grid=cv_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "csv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = csv.best_params_\n",
    "\n",
    "# Display the best parameters\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.6,\n",
    "    'max_delta_step': 1,\n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost Classifier\n",
    "xgb_model = XGBClassifier(**final_params)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_preds, average='macro')\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "valid_preds = xgb_model.predict(X_valid)\n",
    "valid_f1 = f1_score(y_valid, valid_preds, average='macro')\n",
    "valid_accuracy = accuracy_score(y_valid, valid_preds)\n",
    "\n",
    "# Display the F1 scores\n",
    "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictinos on test data\n",
    "test_preds = xgb_model.predict(test_data)\n",
    "\n",
    "# convert predictions to original target values\n",
    "target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "test_preds = pd.Series(test_preds).map(target_mapping)\n",
    "\n",
    "# make csv file for submission\n",
    "submission = pd.DataFrame({\n",
    "    'UID': test_data.index,\n",
    "    'Target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning for XGBoost to get max F1 macro score on validation set \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_fname, predictions_fname):\n",
    "    # Load the test data\n",
    "    test_data = pd.read_csv(test_fname)\n",
    "\n",
    "    predictions = np.array([random.choice([0, 1, 2]) for _ in range(len(test_data))])\n",
    "\n",
    "    # map 0 -> low, 1 -> medium, 2 -> high\n",
    "    predictions = np.array(['low', 'medium', 'high'])[predictions]\n",
    "\n",
    "    # Save the predictions to CSV file containing UID and Target columns\n",
    "    pd.DataFrame({\n",
    "        'UID': test_data['UID'],\n",
    "        'Target': predictions\n",
    "    }).to_csv(predictions_fname, index=False)\n",
    "\n",
    "\n",
    "# make_predictions(\"test.csv\", \"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
